{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e943528a",
   "metadata": {},
   "source": [
    "# Understanding Context Windows (Hands-On Tutorial)\n",
    "Instructor: Zion Pibowei\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1uEjuwRbg8hfHWYj2yS78tOzw2zACRf0d?usp=sharing) [![Watch Video](https://img.shields.io/badge/Watch%20Video-4285F4?logo=googledrive&logoColor=white)](https://drive.google.com/file/d/168Fj31nd2NiOGwVZ28E0VyuTMfKAGuxD/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1763dfa",
   "metadata": {},
   "source": [
    "### 1. What is a Context Window?\n",
    "A context window is the maximum amount of text (measured in tokens) that a language model can process at one time. Think of it as the model's \"working memory\" - everything it can \"see\" and reason about in a single interaction.\n",
    "\n",
    "#### Key Concepts\n",
    "**Tokens:** Not the same as words! A token is typically:\n",
    "\n",
    "- 1 token ≈ 4 characters in English\n",
    "- 1 token ≈ ¾ of a word on average\n",
    "- \"Hello world!\" = ~3 tokens\n",
    "- \"Understanding\" = ~2-3 tokens\n",
    "\n",
    "**Context Window Size:** Different models have different limits\n",
    "\n",
    "- GPT-3.5-Turbo: 16K tokens (~12,000 words)\n",
    "- GPT-4: 8K-128K tokens (varies by version)\n",
    "- Claude 3.5 Sonnet: 200K tokens (~150,000 words)\n",
    "- Llama 3.1: 128K tokens\n",
    "- Gemini 1.5 Pro: 2M tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33698e3a",
   "metadata": {},
   "source": [
    "### 2. Basic Context Window Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ec63d",
   "metadata": {},
   "source": [
    "#### Counting Tokens\n",
    "Let's understand how text translates to tokens using `tiktoken`, OpenAI's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b00d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Hi'\n",
      "Tokens: 1\n",
      "Chars: 2, Ratio: 2.00 chars/token\n",
      "\n",
      "Text: 'Hello, world!'\n",
      "Tokens: 4\n",
      "Chars: 13, Ratio: 3.25 chars/token\n",
      "\n",
      "Text: 'The quick brown fox jumps over the lazy dog.'\n",
      "Tokens: 10\n",
      "Chars: 44, Ratio: 4.40 chars/token\n",
      "\n",
      "Text: 'Supercalifragilisticexpialidocious'\n",
      "Tokens: 11\n",
      "Chars: 34, Ratio: 3.09 chars/token\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Test different texts\n",
    "examples = [\n",
    "    \"Hi\",\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Supercalifragilisticexpialidocious\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    tokens = count_tokens(text)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Chars: {len(text)}, Ratio: {len(text)/tokens:.2f} chars/token\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9d61a",
   "metadata": {},
   "source": [
    "#### What Fits into Context?\n",
    "Let's create a function to check if a given content fits in a context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f429a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fits: True\n",
      "Using 0.8% of context window\n",
      "Remaining tokens: 3562\n"
     ]
    }
   ],
   "source": [
    "def will_it_fit(system_prompt, conversation_history, max_context=4096):\n",
    "    \"\"\"Check if conversation fits in context window\"\"\"\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Count system prompt\n",
    "    total_tokens += count_tokens(system_prompt)\n",
    "    \n",
    "    # Count conversation\n",
    "    for message in conversation_history:\n",
    "        total_tokens += count_tokens(message[\"content\"])\n",
    "        total_tokens += 4  # Message formatting overhead\n",
    "    \n",
    "    # Reserve space for response (estimate)\n",
    "    response_buffer = 500\n",
    "    \n",
    "    remaining = max_context - total_tokens - response_buffer\n",
    "    fits = remaining > 0\n",
    "    \n",
    "    return {\n",
    "        \"fits\": fits,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"max_context\": max_context,\n",
    "        \"remaining\": remaining,\n",
    "        \"percentage_used\": (total_tokens / max_context) * 100\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "system = \"You are a helpful assistant.\"\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Python is a programming language...\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I learn it?\"}\n",
    "]\n",
    "\n",
    "result = will_it_fit(system, history, max_context=4096)\n",
    "print(f\"Fits: {result['fits']}\")\n",
    "print(f\"Using {result['percentage_used']:.1f}% of context window\")\n",
    "print(f\"Remaining tokens: {result['remaining']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98fa01",
   "metadata": {},
   "source": [
    "### 3. Context Window Limitations\n",
    "#### Catstrophic Forgetting - The \"Lost in the Middle\" Problem\n",
    "Models pay more attention to the beginning and end of context, but can \"forget\" information in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3118589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_needle_in_haystack_test(needle_position=\"middle\", bulk_size=20):\n",
    "    \"\"\"\n",
    "    Create a test where important info is placed at different positions\n",
    "    \"\"\"\n",
    "    \n",
    "    needle = \"The secret password is BLUE_ELEPHANT_2024.\"\n",
    "    \n",
    "    filler = [\n",
    "        \"Here's some information about various topics.\",\n",
    "        \"Climate change is affecting global temperatures.\",\n",
    "        \"The stock market fluctuates based on many factors.\",\n",
    "        \"Machine learning models require large datasets.\",\n",
    "        \"Coffee is one of the most popular beverages worldwide.\",\n",
    "    ] * bulk_size  # Repeat to create bulk\n",
    "    \n",
    "    if needle_position == \"start\":\n",
    "        content = [needle] + filler\n",
    "    elif needle_position == \"middle\":\n",
    "        mid = len(filler) // 2\n",
    "        content = filler[:mid] + [needle] + filler[mid:]\n",
    "    else:  # end\n",
    "        content = filler + [needle]\n",
    "    \n",
    "    prompt = \"\\n\".join(content) + \"\\n\\nQuestion: What is the secret password?\"\n",
    "    \n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff10a92e",
   "metadata": {},
   "source": [
    "Test below with different LLMs. Iterate between different LLMs, context window sizes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "634e7ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The secret password is BLUE_ELEPHANT_2024.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "models = [\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"openai/gpt-oss-120b\",\n",
    "    \"openai/gpt-oss-120b\",\n",
    "]\n",
    "\n",
    "\n",
    "# Test prompts\n",
    "prompt_start = create_needle_in_haystack_test(\"start\", bulk_size=20)\n",
    "prompt_middle = create_needle_in_haystack_test(\"middle\", bulk_size=230)\n",
    "prompt_end = create_needle_in_haystack_test(\"end\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{prompt_middle}\"\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=models[1],\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2c86e",
   "metadata": {},
   "source": [
    "**Key Insight**\n",
    "\n",
    "Models have \"recency bias\" (remember recent info) and \"primacy bias\" (remember early info), but struggle with middle content in long contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab84e90",
   "metadata": {},
   "source": [
    "### 4. Managing Context Windows\n",
    "#### Sliding Window Approach\n",
    "Keep only the most recent N messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7955c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original conversation history: 100 messages\n",
      "Trimmed conversation history: 5 messages\n"
     ]
    }
   ],
   "source": [
    "def sliding_window(conversation_history, max_messages=10):\n",
    "    \"\"\"Keep only the last N messages\"\"\"\n",
    "    if len(conversation_history) <= max_messages:\n",
    "        return conversation_history\n",
    "    \n",
    "    # Always keep system message if it exists\n",
    "    if conversation_history[0][\"role\"] == \"system\":\n",
    "        return [conversation_history[0]] + conversation_history[-(max_messages-1):]\n",
    "    \n",
    "    return conversation_history[-max_messages:]\n",
    "\n",
    "# Test\n",
    "long_conversation_nested = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Message {i}\"}\n",
    "    ]\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "# flatten\n",
    "long_conversation = sum(long_conversation_nested, [])\n",
    "\n",
    "trimmed = sliding_window(long_conversation, max_messages=5)\n",
    "\n",
    "print(f\"Original conversation history: {len(long_conversation)} messages\")\n",
    "print(f\"Trimmed conversation history: {len(trimmed)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f05e2",
   "metadata": {},
   "source": [
    "#### Smart Summarization\n",
    "Summarize old messages to preserve context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4c58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_compress(conversation_history, llm_function):\n",
    "    \"\"\"\n",
    "    Summarize older messages while keeping recent ones intact\n",
    "    \"\"\"\n",
    "    if len(conversation_history) < 10:\n",
    "        return conversation_history\n",
    "    \n",
    "    # Keep system prompt and last 4 messages\n",
    "    recent_messages = conversation_history[-4:]\n",
    "    old_messages = conversation_history[:-4]\n",
    "    \n",
    "    # Create summary prompt\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{msg['role']}: {msg['content']}\" \n",
    "        for msg in old_messages\n",
    "    ])\n",
    "    \n",
    "    summary_prompt = f\"\"\"Summarize this conversation concisely, preserving key facts and context:\n",
    "\n",
    "{conversation_text}\n",
    "\n",
    "Provide a brief summary (2-3 sentences):\"\"\"\n",
    "    \n",
    "    summary = llm_function(summary_prompt)\n",
    "    \n",
    "    # Return compressed history\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"}\n",
    "    ] + recent_messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f562b6",
   "metadata": {},
   "source": [
    "#### Token-Based Truncation\n",
    "Truncate based on actual token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310274d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 480 messages\n",
      "Removed 520 messages\n",
      "Total tokens: 3592\n"
     ]
    }
   ],
   "source": [
    "def truncate_to_fit(conversation, max_tokens=4000, buffer=500):\n",
    "    \"\"\"\n",
    "    Remove oldest messages until conversation fits in context window\n",
    "    \"\"\"\n",
    "    target_tokens = max_tokens - buffer\n",
    "    system_message = None\n",
    "    messages = conversation.copy()\n",
    "    \n",
    "    # Extract system message if present\n",
    "    if messages and messages[0][\"role\"] == \"system\":\n",
    "        system_message = messages.pop(0)\n",
    "    \n",
    "    # Count from newest to oldest\n",
    "    total_tokens = 0\n",
    "    kept_messages = []\n",
    "    \n",
    "    for message in reversed(messages):\n",
    "        msg_tokens = count_tokens(message[\"content\"]) + 4\n",
    "        if total_tokens + msg_tokens <= target_tokens:\n",
    "            kept_messages.insert(0, message)\n",
    "            total_tokens += msg_tokens\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Add system message back\n",
    "    if system_message:\n",
    "        kept_messages.insert(0, system_message)\n",
    "    \n",
    "    removed = len(messages) - len(kept_messages) + (1 if system_message else 0)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": kept_messages,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"removed_count\": removed\n",
    "    }\n",
    "\n",
    "# Test\n",
    "conversation_history = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Message {i}\"}\n",
    "    ]\n",
    "    for i in range(500)\n",
    "]\n",
    "# flatten\n",
    "conversation_history = sum(conversation_history, [])\n",
    "\n",
    "result = truncate_to_fit(conversation_history, max_tokens=4096)\n",
    "print(f\"Kept {len(result['messages'])} messages\")\n",
    "print(f\"Removed {result['removed_count']} messages\")\n",
    "print(f\"Total tokens: {result['total_tokens']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-v0 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
